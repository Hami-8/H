{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86578f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1adcd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.35.0-py3-none-any.whl.metadata (123 kB)\n",
      "     -------------------------------------- 123.1/123.1 kB 1.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\18094\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\18094\\anaconda3\\lib\\site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\18094\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\18094\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\18094\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\18094\\anaconda3\\lib\\site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: requests in c:\\users\\18094\\anaconda3\\lib\\site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in c:\\users\\18094\\anaconda3\\lib\\site-packages (from transformers) (0.14.1)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.4.0-cp39-none-win_amd64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\18094\\anaconda3\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\18094\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\18094\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\18094\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\18094\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\18094\\anaconda3\\lib\\site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\18094\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\18094\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\18094\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
      "   ---------------------------------------- 7.9/7.9 MB 11.8 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.0-cp39-none-win_amd64.whl (277 kB)\n",
      "   ---------------------------------------- 277.2/277.2 kB 8.6 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, transformers\n",
      "Successfully installed safetensors-0.4.0 transformers-4.35.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca738869",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import BertTokenizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8c704ca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#定义一个载入原始数据的函数\n",
    "def load_data(base_path):\n",
    "    paths = os.listdir(base_path)   #获取base_path目录下的所有文件和文件夹的名称，并将它们存储在paths列表中\n",
    "    result = []   \n",
    "    for path in paths:   #遍历paths列表中的每个元素，即每个文件名\n",
    "        with open(os.path.join(base_path, path), 'r', encoding='utf-8') as f:   #使用open()函数打开当前文件名拼接而成的完整路径，并指定以只读模式打开文件。同时，设置文件的编码格式为UTF-8，并将打开的文件对象赋值给变量f\n",
    "            result.append(f.readline())   #从文件中读取一行数据，并将其添加到result列表中\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "321a3baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义一个读取数据的函数，将读取到的数据转化为datasets.Dataset\n",
    "def get_dataset(base_path):\n",
    "    pos_data = load_data(os.path.join(base_path, 'pos'))\n",
    "    neg_data = load_data(os.path.join(base_path, 'neg'))   #分别调用了之前定义的load_data函数，分别读取了base_path/pos和base_path/neg目录下的数据，并将其存储在pos_data和neg_data变量中\n",
    "    texts = pos_data + neg_data\n",
    "    labels = [[1., 0.]]*len(pos_data) + [[0., 1.]] * len(neg_data)   #创建了一个标签列表labels，其中[[1., 0.]]*len(pos_data)表示将[1., 0.]这个标签重复len(pos_data)次，[[0., 1.]] * len(neg_data)表示将[0., 1.]这个标签重复len(neg_data)次，然后将这两部分标签列表合并\n",
    "    dataset = Dataset.from_dict({'texts':texts, 'labels':labels})   #使用Dataset.from_dict()方法将文本数据和标签数据组装成一个字典，并使用该字典创建了一个datasets.Dataset对象\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c9c7549",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#读取数据\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mget_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:/Users/18094/Desktop/project/aclImdb_v1/aclImdb/train/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m get_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/18094/Desktop/project/aclImdb_v1/aclImdb/test/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36mget_dataset\u001b[1;34m(base_path)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_dataset\u001b[39m(base_path):\n\u001b[1;32m----> 3\u001b[0m     pos_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpos\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     neg_data \u001b[38;5;241m=\u001b[39m load_data(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      5\u001b[0m     texts \u001b[38;5;241m=\u001b[39m pos_data \u001b[38;5;241m+\u001b[39m neg_data\n",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(base_path)\u001b[0m\n\u001b[0;32m      4\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m paths:\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      7\u001b[0m         result\u001b[38;5;241m.\u001b[39mappend(f\u001b[38;5;241m.\u001b[39mreadline())\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\codecs.py:309\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.__init__\u001b[1;34m(self, errors)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBufferedIncrementalDecoder\u001b[39;00m(IncrementalDecoder):\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;124;03m    This subclass of IncrementalDecoder can be used as the baseclass for an\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;124;03m    incremental decoder if the decoder must be able to handle incomplete\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;124;03m    byte sequences.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 309\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    310\u001b[0m         IncrementalDecoder\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, errors)\n\u001b[0;32m    311\u001b[0m         \u001b[38;5;66;03m# undecoded input that is kept between calls to decode()\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#读取数据\n",
    "train_dataset = get_dataset('C:/Users/18094/Desktop/project/aclImdb_v1/aclImdb/train/')\n",
    "test_dataset = get_dataset('C:/Users/18094/Desktop/project/aclImdb_v1/aclImdb/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4d6fd0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['texts', 'labels'],\n",
      "    num_rows: 25000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1ab912a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cache_dir='C:/Users/18094/Desktop/project/transformersModels/bert-base-uncased2'\n",
    "tokenizer = BertTokenizer.from_pretrained(cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bb901b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9a10f2bd7614dfda25a5440cf711b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25503baa2e5b4546a156919e4411b52a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#转化数据，转化成模型可以接受的形式\n",
    "# 设置最大长度\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# 使用文本标记器对texts进行编码\n",
    "train_dataset = train_dataset.map(lambda e: tokenizer(e['texts'], truncation=True, padding='max_length', max_length=MAX_LENGTH), batched=True)#map:对整个训练集进行映射操作；lambda e:匿名函数，接受一个参数e；truncation=True表示进行截断处理，padding='max_length'表示进行填充操作，max_length=MAX_LENGTH表示限制最大长度为512个token\n",
    "test_dataset = test_dataset.map(lambda e: tokenizer(e['texts'], truncation=True, padding='max_length', max_length=MAX_LENGTH), batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c0a8cf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'texts': Value(dtype='string', id=None), 'labels': Sequence(feature=Value(dtype='float64', id=None), length=-1, id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(print(train_dataset.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c8990cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "Tried to overwrite C:\\Users\\18094\\Desktop\\project\\data\\train_dataset but a dataset can't overwrite itself.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#保存数据至本地\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_to_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:/Users/18094/Desktop/project/data/train_dataset\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m test_dataset\u001b[38;5;241m.\u001b[39msave_to_disk(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/18094/Desktop/project/data/test_dataset\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py:1468\u001b[0m, in \u001b[0;36mDataset.save_to_disk\u001b[1;34m(self, dataset_path, fs, max_shard_size, num_shards, num_proc, storage_options)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     \u001b[38;5;66;03m# Check that the dataset doesn't overwrite iself. It can cause a permission error on Windows and a segfault on linux.\u001b[39;00m\n\u001b[0;32m   1467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m Path(dataset_path)\u001b[38;5;241m.\u001b[39mresolve() \u001b[38;5;129;01min\u001b[39;00m parent_cache_files_paths:\n\u001b[1;32m-> 1468\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mPermissionError\u001b[39;00m(\n\u001b[0;32m   1469\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to overwrite \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPath(dataset_path)\u001b[38;5;241m.\u001b[39mresolve()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but a dataset can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt overwrite itself.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1470\u001b[0m         )\n\u001b[0;32m   1471\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1472\u001b[0m     fs\u001b[38;5;241m.\u001b[39mmakedirs(dataset_path, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mPermissionError\u001b[0m: Tried to overwrite C:\\Users\\18094\\Desktop\\project\\data\\train_dataset but a dataset can't overwrite itself."
     ]
    }
   ],
   "source": [
    "#保存数据至本地\n",
    "train_dataset.save_to_disk('C:/Users/18094/Desktop/project/data/train_dataset')\n",
    "test_dataset.save_to_disk('C:/Users/18094/Desktop/project/data/test_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db66febe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练模型\n",
    "#导入必要的库\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments, BertConfig\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8be71f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at C:/Users/18094/Desktop/project/transformersModels/bert-base-uncased2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 将num_labels设置为2\n",
    "model = BertForSequenceClassification.from_pretrained('C:/Users/18094/Desktop/project/transformersModels/bert-base-uncased2', num_labels=2) #num_labels=2表示输出类别数量为2，二分类任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14ed64a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载处理好的数据\n",
    "train_dataset = Dataset.load_from_disk('C:/Users/18094/Desktop/project/data/train_dataset')\n",
    "test_dataset = Dataset.load_from_disk('C:/Users/18094/Desktop/project/data/test_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2477c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#冻结BERT参数\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False     #冻结了对应参数的梯度计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a414371",
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建trainer\n",
    "# 训练超参配置\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='C:/Users/18094/Desktop/project/my_results',          #结果输出地址\n",
    "    num_train_epochs=20,              # 训练总批次\n",
    "    per_device_train_batch_size=64,  # 训练批大小\n",
    "    per_device_eval_batch_size=64,   # 评估批大小\n",
    "    logging_dir='C:/Users/18094/Desktop/project/my_logs',            # 日志存储位置\n",
    ")\n",
    "\n",
    "# 创建Trainer\n",
    "trainer = Trainer(\n",
    "    model=model.to('cuda'),              # 需要训练的模型，转移至gpu上\n",
    "    args=training_args,                  # 训练参数\n",
    "    train_dataset=train_dataset,         # training dataset 训练集\n",
    "    eval_dataset=test_dataset,           # evaluation dataset 测试集\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2668c232",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3714' max='7820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3714/7820 3:56:53 < 4:22:01, 0.26 it/s, Epoch 9.50/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.679300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.661700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.648300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.638500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.630400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.622400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.617500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 开始训练\n",
    "trainer.train()\n",
    "# 开始评估模型\n",
    "trainer.evaluate()\n",
    "\n",
    "# 保存模型 会保存到配置的output_dir处\n",
    "trainer.save_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0054510a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "11.6\n"
     ]
    }
   ],
   "source": [
    "#加载模型\n",
    "output_config_file = 'C:/Users/18094/Desktop/project/my_results/config.json'   #模型配置文件\n",
    "output_model_file = 'C:/Users/18094/Desktop/project/my_results/pytorch_model.bin'  #模型数据文件\n",
    "\n",
    "config = BertConfig.from_json_file(output_config_file)   #从指定的JSON文件中加载BERT模型的配置\n",
    "model = BertForSequenceClassification(config)   #创建一个BertForSequenceClassification模型，并将上一步加载的配置传递给它\n",
    "state_dict = torch.load(output_model_file)   #加载保存在output_model_file路径下的模型权重参数\n",
    "model.load_state_dict(state_dict)   #将参数加载到之前创建的BERT模型中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befc9890",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
